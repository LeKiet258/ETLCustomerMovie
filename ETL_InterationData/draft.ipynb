{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext() # dòng này phải nằm trước dòng dưới\n",
    "spark = SparkSession.builder.appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test create fact table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source table\n",
    "df_dim_payment_transaction = spark.read.parquet(\"../temp/dim_payment_transaction/*.parquet\")\n",
    "    \n",
    "df_dim_payment_type = spark.read.parquet(\"../temp/dim_payment_type.parquet\")\n",
    "\n",
    "df_dim_account = spark.read.parquet(\"../temp/dim_account.parquet\")\n",
    "\n",
    "df_dim_account_type = spark.read.parquet(\"../temp/dim_account_type.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+----+--------+-------------+----------------------------+\n",
      "|date      |is_holiday|quarter|year|date_key|week_of_month|holiday_name                |\n",
      "+----------+----------+-------+----+--------+-------------+----------------------------+\n",
      "|2015-01-01|true      |1      |2015|20150101|1            |International New Year's Day|\n",
      "|2015-01-02|false     |1      |2015|20150102|1            |work day                    |\n",
      "|2015-01-03|false     |1      |2015|20150103|1            |work day                    |\n",
      "|2015-01-04|false     |1      |2015|20150104|1            |work day                    |\n",
      "|2015-01-05|false     |1      |2015|20150105|2            |work day                    |\n",
      "|2015-01-06|false     |1      |2015|20150106|2            |work day                    |\n",
      "|2015-01-07|false     |1      |2015|20150107|2            |work day                    |\n",
      "|2015-01-08|false     |1      |2015|20150108|2            |work day                    |\n",
      "|2015-01-09|false     |1      |2015|20150109|2            |work day                    |\n",
      "|2015-01-10|false     |1      |2015|20150110|2            |work day                    |\n",
      "|2015-01-11|false     |1      |2015|20150111|2            |work day                    |\n",
      "|2015-01-12|false     |1      |2015|20150112|3            |work day                    |\n",
      "|2015-01-13|false     |1      |2015|20150113|3            |work day                    |\n",
      "|2015-01-14|false     |1      |2015|20150114|3            |work day                    |\n",
      "|2015-01-15|false     |1      |2015|20150115|3            |work day                    |\n",
      "|2015-01-16|false     |1      |2015|20150116|3            |work day                    |\n",
      "|2015-01-17|false     |1      |2015|20150117|3            |work day                    |\n",
      "|2015-01-18|false     |1      |2015|20150118|3            |work day                    |\n",
      "|2015-01-19|false     |1      |2015|20150119|4            |work day                    |\n",
      "|2015-01-20|false     |1      |2015|20150120|4            |work day                    |\n",
      "+----------+----------+-------+----+--------+-------------+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from calendar import monthcalendar\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import date\n",
    "import holidays\n",
    "\n",
    "def date_range(start_date, end_date):\n",
    "    start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    delta = timedelta(days=1)\n",
    "    dates_list = []\n",
    "    while start_date <= end_date:\n",
    "        dates_list.append(start_date.strftime('%Y-%m-%d'))\n",
    "        start_date += delta\n",
    "    return dates_list\n",
    "\n",
    "start_date = '2015-01-01'\n",
    "end_date = '2024-12-31'\n",
    "\n",
    "date_list = date_range(start_date, end_date)\n",
    "vn_holidays = holidays.VN()  # this is a dict\n",
    "\n",
    "def is_holiday(date):\n",
    "    return date in vn_holidays\n",
    "\n",
    "def get_holiday_name(date):\n",
    "    return vn_holidays.get(date)\n",
    "\n",
    "def get_week_of_month(year, month, day):\n",
    "    return next(\n",
    "        (\n",
    "            week_number\n",
    "            for week_number, days_of_week in enumerate(monthcalendar(year, month), start=1)\n",
    "            if day in days_of_week\n",
    "        ),\n",
    "        None,\n",
    "    )\n",
    "    \n",
    "udf_is_holiday = udf(is_holiday, BooleanType())\n",
    "udf_get_week_of_month = udf(get_week_of_month)\n",
    "udf_get_holiday_name = udf(get_holiday_name, StringType())\n",
    "\n",
    "df_dim_date = spark.createDataFrame([(date,) for date in date_list], ['date']) \\\n",
    "    .withColumn('date', to_date(col('date'), 'yyyy-MM-dd')) \\\n",
    "    .withColumn(\"is_holiday\", udf_is_holiday(\"date\")) \\\n",
    "    .withColumn(\"quarter\", quarter(\"date\")) \\\n",
    "    .withColumn(\"year\", date_format(\"date\", \"yyyy\")) \\\n",
    "    .withColumn(\"date_key\", date_format(\"date\", \"yyyyMMdd\")) \\\n",
    "    .withColumn(\"week_of_month\", udf_get_week_of_month(year(col('date')), month(col('date')), dayofmonth(col('date')))) \\\n",
    "    .withColumn('holiday_name', when(col('is_holiday') == True, udf_get_holiday_name(col('date'))).otherwise(lit('work day')))\n",
    "    \n",
    "\n",
    "df_dim_date.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_payment_transaction_full = df_dim_payment_transaction.join(df_dim_payment_type, df_dim_payment_transaction['payment_code'] == df_dim_payment_type['type_code']) \\\n",
    "    .withColumn('transaction_date', to_date('transaction_time'))\n",
    "\n",
    "df_payment_transaction_full.createOrReplaceTempView('dim_payment_transaction')\n",
    "df_dim_date.createOrReplaceTempView('dim_date')\n",
    "df_dim_account.createOrReplaceTempView('dim_account')\n",
    "df_dim_account_type.createOrReplaceTempView('dim_account_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------------------------------+---------------------------------------------+-----------------+-----------------------------+----------------------+----------------------+-------------------------+-------------------+------------------------------+-------------------+----------------------------+\n",
      "|date_key|cust_id                                      |acc_id                                       |account_type_name|account_no_transactions_daily|account_daily_spending|account_accum_spending|cust_no_transaction_daily|cust_daily_spending|cust_daily_payment_type       |cust_accum_spending|cust_avgerage_daily_spending|\n",
      "+--------+---------------------------------------------+---------------------------------------------+-----------------+-----------------------------+----------------------+----------------------+-------------------------+-------------------+------------------------------+-------------------+----------------------------+\n",
      "|20230801|011a433974c5e6461a5b362b00c3422d2102689346752|bf9085585d9d7ef8e0e37eda844d93332102839811376|BB               |5                            |182.0                 |182.0                 |5                        |182.0              |[TRANSFER, TECHNICAL, ONLINE] |182.0              |67.16666666666667           |\n",
      "|20230801|011a433974c5e6461a5b362b00c3422d2102689346752|bf9085585d9d7ef8e0e37eda844d93332102839811376|BB               |5                            |182.0                 |182.0                 |5                        |182.0              |[TRANSFER, TECHNICAL, ONLINE] |403.0              |67.16666666666667           |\n",
      "|20230802|011a433974c5e6461a5b362b00c3422d2102689346752|bf9085585d9d7ef8e0e37eda844d93332102839811376|BB               |1                            |221.0                 |403.0                 |1                        |221.0              |[TRANSFER]                    |403.0              |67.16666666666667           |\n",
      "|20230802|011a433974c5e6461a5b362b00c3422d2102689346752|bf9085585d9d7ef8e0e37eda844d93332102839811376|BB               |1                            |221.0                 |403.0                 |1                        |221.0              |[TRANSFER]                    |182.0              |67.16666666666667           |\n",
      "|20230801|012a26c4676b2159b41f997f6166816c2102691280256|929127a4e37840ff6a720e199548ed2d2102841447856|RBG              |1                            |-57.0                 |-57.0                 |8                        |-576.0             |[RESTAURANT, TRANSFER, ONLINE]|-2632.0            |-202.46153846153845         |\n",
      "|20230801|012a26c4676b2159b41f997f6166816c2102691280256|e7a442a74fe4eda2db36fc0d71b02d142102841187824|BB               |4                            |-463.0                |-463.0                |8                        |-576.0             |[RESTAURANT, TRANSFER, ONLINE]|-2632.0            |-202.46153846153845         |\n",
      "|20230801|012a26c4676b2159b41f997f6166816c2102691280256|929127a4e37840ff6a720e199548ed2d2102841447856|RBG              |1                            |-57.0                 |-57.0                 |8                        |-576.0             |[RESTAURANT, TRANSFER, ONLINE]|-1061.0            |-202.46153846153845         |\n",
      "|20230801|012a26c4676b2159b41f997f6166816c2102691280256|e7a442a74fe4eda2db36fc0d71b02d142102841187824|BB               |4                            |-463.0                |-463.0                |8                        |-576.0             |[RESTAURANT, TRANSFER, ONLINE]|-1061.0            |-202.46153846153845         |\n",
      "|20230801|012a26c4676b2159b41f997f6166816c2102691280256|929127a4e37840ff6a720e199548ed2d2102841447856|RBG              |1                            |-57.0                 |-57.0                 |8                        |-576.0             |[RESTAURANT, TRANSFER, ONLINE]|-576.0             |-202.46153846153845         |\n",
      "|20230801|012a26c4676b2159b41f997f6166816c2102691280256|e7a442a74fe4eda2db36fc0d71b02d142102841187824|BB               |4                            |-463.0                |-463.0                |8                        |-576.0             |[RESTAURANT, TRANSFER, ONLINE]|-576.0             |-202.46153846153845         |\n",
      "|20230801|012a26c4676b2159b41f997f6166816c2102691280256|4fd030e0371e1fbdf7deaba0f1e649412102836925776|RBG              |3                            |-56.0                 |-56.0                 |8                        |-576.0             |[RESTAURANT, TRANSFER, ONLINE]|-2632.0            |-202.46153846153845         |\n",
      "|20230801|012a26c4676b2159b41f997f6166816c2102691280256|4fd030e0371e1fbdf7deaba0f1e649412102836925776|RBG              |3                            |-56.0                 |-56.0                 |8                        |-576.0             |[RESTAURANT, TRANSFER, ONLINE]|-1061.0            |-202.46153846153845         |\n",
      "|20230801|012a26c4676b2159b41f997f6166816c2102691280256|4fd030e0371e1fbdf7deaba0f1e649412102836925776|RBG              |3                            |-56.0                 |-56.0                 |8                        |-576.0             |[RESTAURANT, TRANSFER, ONLINE]|-576.0             |-202.46153846153845         |\n",
      "|20230802|012a26c4676b2159b41f997f6166816c2102691280256|929127a4e37840ff6a720e199548ed2d2102841447856|RBG              |2                            |-485.0                |-542.0                |2                        |-485.0             |[NORMAL, ONLINE]              |-2632.0            |-202.46153846153845         |\n",
      "|20230802|012a26c4676b2159b41f997f6166816c2102691280256|929127a4e37840ff6a720e199548ed2d2102841447856|RBG              |2                            |-485.0                |-542.0                |2                        |-485.0             |[NORMAL, ONLINE]              |-1061.0            |-202.46153846153845         |\n",
      "|20230802|012a26c4676b2159b41f997f6166816c2102691280256|929127a4e37840ff6a720e199548ed2d2102841447856|RBG              |2                            |-485.0                |-542.0                |2                        |-485.0             |[NORMAL, ONLINE]              |-576.0             |-202.46153846153845         |\n",
      "|20230803|012a26c4676b2159b41f997f6166816c2102691280256|4fd030e0371e1fbdf7deaba0f1e649412102836925776|RBG              |1                            |-554.0                |-610.0                |3                        |-1571.0            |[TECHNICAL, NORMAL]           |-576.0             |-202.46153846153845         |\n",
      "|20230803|012a26c4676b2159b41f997f6166816c2102691280256|e7a442a74fe4eda2db36fc0d71b02d142102841187824|BB               |2                            |-1017.0               |-1480.0               |3                        |-1571.0            |[TECHNICAL, NORMAL]           |-2632.0            |-202.46153846153845         |\n",
      "|20230803|012a26c4676b2159b41f997f6166816c2102691280256|e7a442a74fe4eda2db36fc0d71b02d142102841187824|BB               |2                            |-1017.0               |-1480.0               |3                        |-1571.0            |[TECHNICAL, NORMAL]           |-1061.0            |-202.46153846153845         |\n",
      "|20230803|012a26c4676b2159b41f997f6166816c2102691280256|e7a442a74fe4eda2db36fc0d71b02d142102841187824|BB               |2                            |-1017.0               |-1480.0               |3                        |-1571.0            |[TECHNICAL, NORMAL]           |-576.0             |-202.46153846153845         |\n",
      "+--------+---------------------------------------------+---------------------------------------------+-----------------+-----------------------------+----------------------+----------------------+-------------------------+-------------------+------------------------------+-------------------+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fact = spark.sql(\"\"\"\n",
    "    with cte_transaction_revenue as (\n",
    "        select transaction_date, a.cust_id,\n",
    "            count(trans_id) as cust_no_transaction_daily,\n",
    "            sum(amount) as cust_daily_spending,\n",
    "            collect_list(distinct type_nm) as cust_daily_payment_type\n",
    "        from dim_payment_transaction pm join dim_account a on pm.acc_id = a.acc_id\n",
    "        group by transaction_date, a.cust_id\n",
    "    ),\n",
    "    cte_cust_accum_revenue as (\n",
    "        select cust_id,\n",
    "            sum(cust_daily_spending) over (partition by cust_id order by transaction_date) as cust_accum_spending\n",
    "        from cte_transaction_revenue\n",
    "    ),\n",
    "    cte_account_payment_summary as (\n",
    "        select pm.transaction_date, cust_id, a.acc_id, at.type_nm as account_type_name,\n",
    "            count(distinct pm.trans_id) as account_no_transactions_daily,\n",
    "            sum(pm.amount) as account_daily_spending\n",
    "        from dim_payment_transaction pm\n",
    "            join dim_account a on pm.acc_id = a.acc_id\n",
    "            join dim_account_type at on a.acc_type = at.type_id\n",
    "        group by pm.transaction_date, a.cust_id, a.acc_id, at.type_nm\n",
    "    ),\n",
    "    cte_account_accum_revenue as (\n",
    "        select transaction_date, cust_id, acc_id, account_daily_spending,\n",
    "            sum(account_daily_spending) over (partition by cust_id, acc_id order by transaction_date) as account_accum_spending\n",
    "        from cte_account_payment_summary\n",
    "    ),\n",
    "    cte_customer_avgerage_daily_spending as (\n",
    "        select month(transaction_date) as month, a.cust_id, \n",
    "            avg(amount) as cust_avgerage_daily_spending\n",
    "        from dim_payment_transaction pm join dim_account a on pm.acc_id = a.acc_id\n",
    "        group by month(transaction_date), a.cust_id\n",
    "    )\n",
    "    \n",
    "    select date_format(a1.transaction_date, 'yyyyMMdd') as date_key, a1.cust_id, a1.acc_id, a1.account_type_name,\n",
    "            a1.account_no_transactions_daily,\n",
    "            a1.account_daily_spending,\n",
    "        a2.account_accum_spending,\n",
    "        c1.cust_no_transaction_daily, c1.cust_daily_spending, c1.cust_daily_payment_type,\n",
    "        c2.cust_accum_spending,\n",
    "        c3.cust_avgerage_daily_spending\n",
    "    from cte_account_payment_summary a1 \n",
    "        join cte_account_accum_revenue a2 on a1.transaction_date = a2.transaction_date and a1.cust_id = a2.cust_id and a1.acc_id = a2.acc_id\n",
    "        join cte_transaction_revenue c1 on a1.transaction_date = c1.transaction_date and a1.cust_id = c1.cust_id \n",
    "        join cte_cust_accum_revenue c2 on c1.cust_id = c2.cust_id \n",
    "        join cte_customer_avgerage_daily_spending c3 on c3.month = month(a1.transaction_date) and c3.cust_id = a1.cust_id\n",
    "    order by a1.cust_id, a1.transaction_date\n",
    "\"\"\")\n",
    "\n",
    "df_fact.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100387"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_payment_transaction_full.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134629"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fact.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong 1 ngày, có acc_id nào thực hiện 2 transaction ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select pm.transaction_date, cust_id, a.acc_id, at.type_nm as account_type_name,\n",
    "        count(distinct pm.trans_id) as account_no_transactions_daily,\n",
    "        sum(pm.amount) as account_daily_spending\n",
    "    from dim_payment_transaction pm\n",
    "        join dim_account a on pm.acc_id = a.acc_id\n",
    "        join dim_account_type at on a.acc_type = at.type_id\n",
    "    group by pm.transaction_date, a.cust_id, a.acc_id, at.type_nm\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.dropTempView(\"dim_payment_transaction\")\n",
    "spark.catalog.dropTempView(\"dim_date\")\n",
    "spark.catalog.dropTempView(\"dim_account\")\n",
    "spark.catalog.dropTempView(\"dim_account_type\")\n",
    "spark.catalog.dropTempView(\"dim_payment_type\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze most search 2 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|user_id|      most_search_t6|      most_search_t7|\n",
      "+-------+--------------------+--------------------+\n",
      "|0003361|         tây hành kỷ|         lộc đỉnh ký|\n",
      "|0005748|   lời nguyền ma lai|chuyen sinh thanh...|\n",
      "|0008207|               CONAN|     TIENG ANH LOP 5|\n",
      "|0017684|sứ mệnh cuói cung...|       bác sĩ yo han|\n",
      "|0019650|                hori| classroomoftheelite|\n",
      "|0027835|lời nói dối của h...|     minh lan truyện|\n",
      "|0041173|detective k: secr...|                anna|\n",
      "|0060714|   học viện ma vương|học viện anh hùng...|\n",
      "|0064645|  unforgettable love|      cá mực hầm mật|\n",
      "|0101498|    công chúa aurora|   nam cung phu nhân|\n",
      "|0103456|phim thương ngày ...|   nữ luật sư kỳ lạ |\n",
      "|0115100|  cặp đôi trái ngược|            the 1000|\n",
      "|0115494|          nhiếp viễn|       đêm giao thừa|\n",
      "|0124079|               JoJo |             phim lẻ|\n",
      "|0139158|           lật ngược|   ngôi sao hàng đầu|\n",
      "|0150958|  vương giả thiên hạ|           one piece|\n",
      "|0151915|          CUOC CHIEN|     TINH HA XAN LAN|\n",
      "|0153643|yêu em từ cái nhì...|hắc bạch vô song ...|\n",
      "|0161303|   TIENG GOI CON TIM|             BERSERK|\n",
      "| 016508|            Bigfoot |    taxi, em tên gì?|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet('../temp/most_search_t67.parquet')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_search_category = spark.read.csv('../temp/map_search_category.csv', header=True)\n",
    "df_search_category.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StopWordsRemover, Tokenizer, NGram, HashingTF, MinHashLSH, RegexTokenizer, SQLTransformer\n",
    "model = Pipeline(stages=[\n",
    "    SQLTransformer(statement=\"SELECT *, lower(Title) lower FROM __THIS__\"),\n",
    "    Tokenizer(inputCol=\"lower\", outputCol=\"token\"),\n",
    "    StopWordsRemover(inputCol=\"token\", outputCol=\"stop\"),\n",
    "    SQLTransformer(statement=\"SELECT *, concat_ws(' ', stop) concat FROM __THIS__\"),\n",
    "    RegexTokenizer(pattern=\"\", inputCol=\"concat\", outputCol=\"char\", minTokenLength=1),\n",
    "    NGram(n=2, inputCol=\"char\", outputCol=\"ngram\"),\n",
    "    HashingTF(inputCol=\"ngram\", outputCol=\"vector\"),\n",
    "    MinHashLSH(inputCol=\"vector\", outputCol=\"lsh\", numHashTables=3)\n",
    "]).fit(lens_ddf)\n",
    "result_lens = model.transform(lens_ddf)\n",
    "result_lens = result_lens.filter(F.size(F.col(\"ngram\")) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_search_category = spark.read.csv('../temp/map_search_category.csv', header=True) \\\n",
    "        .withColumnRenamed('Column1', 'category')\n",
    "\n",
    "df_search_category.createOrReplaceTempView('cte_category')\n",
    "\n",
    "df_tmp = spark.sql(\"\"\"\n",
    "    with cte_mismatch_category as (\n",
    "        SELECT t1.text as text1, t2.text as text2, \n",
    "            t1.category as cat1, t2.category as cat2, \n",
    "            levenshtein(t1.text, t2.text) as similarity\n",
    "        FROM cte_category t1 JOIN cte_category t2 on t1.text != t2.text\n",
    "        where levenshtein(t1.text, t2.text) < 3\n",
    "            and t1.category != t2.category\n",
    "        --order by levenshtein(t1.text, t2.text) desc\n",
    "    )\n",
    "    \n",
    "    select *\n",
    "    from cte_mismatch_category\n",
    "\"\"\")\n",
    "\n",
    "print(df_tmp.count())\n",
    "df_tmp.show(truncate=False)\n",
    "\n",
    "spark.catalog.dropTempView(\"cte_category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_search_category = spark.read.csv('../temp/map_search_category.csv', header=True) \\\n",
    "        .withColumnRenamed('Column1', 'category')\n",
    "\n",
    "df_search_category.createOrReplaceTempView('cte_category')\n",
    "\n",
    "df_tmp = spark.sql(\"\"\"\n",
    "    with cte_mismatch_category as (\n",
    "        SELECT t1.text as text1, t2.text as text2, \n",
    "            t1.category as cat1, t2.category as cat2, \n",
    "            levenshtein(t1.text, t2.text) as similarity,\n",
    "            sort_array(array(t1.text, t2.text)) AS sorted_array\n",
    "        FROM cte_category t1 JOIN cte_category t2 on t1.text != t2.text\n",
    "        where levenshtein(t1.text, t2.text) < 3\n",
    "            and t1.category != t2.category\n",
    "        order by levenshtein(t1.text, t2.text) desc\n",
    "    ),\n",
    "    cte_replace_mismatched_category as (\n",
    "        select text1, text2, cat1, cat1 as cat2 \n",
    "        from (\n",
    "            select row_number() over (partition by sorted_array order by text1) as rn,\n",
    "                text1, text2, cat1, cat2\n",
    "            from cte_mismatch_category \n",
    "        ) \n",
    "        where rn = 1\n",
    "    ),\n",
    "    cte_reduced_mismatch_category as (\n",
    "        select text, category\n",
    "        from (\n",
    "            select text, category,\n",
    "                row_number() over (partition by text order by category) as rn --need distinct text\n",
    "            from (\n",
    "                (select text1 as text, cat1 as category from cte_replace_mismatched_category)\n",
    "                union all \n",
    "                (select text2 as text, cat2 as category from cte_replace_mismatched_category)\n",
    "            )\n",
    "        ) t\n",
    "        where rn = 1\n",
    "    )\n",
    "    \n",
    "    select text, label, category as Column1 from (\n",
    "        select *\n",
    "        from cte_category\n",
    "        where text not in (select text from cte_reduced_mismatch_category)\n",
    "    ) t union all (\n",
    "        select text, -1 as label, category \n",
    "        from cte_reduced_mismatch_category\n",
    "    )\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# select * from cte_mismatch_category t1\n",
    "# where t1.text1_text2 not in (\n",
    "#     select t2.text1_text2\n",
    "#     from cte_mismatch_category t2 \n",
    "#     where t2.text2_text1 = t1.text1_text2\n",
    "# )\n",
    "\n",
    "print(df_tmp.count())\n",
    "df_tmp.show(truncate=False)\n",
    "\n",
    "spark.catalog.dropTempView(\"cte_category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp.coalesce(1).write.mode('overwrite').csv('../temp/map_search_category2.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp.where(\"text2 = 'mộng hoa lục'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_search_text = df.select('most_search_t6').union(df.select('most_search_t7')) \\\n",
    "    .distinct() \\\n",
    "    .collect() \n",
    "    \n",
    "all_search_text = [row[0] for row in all_search_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_search_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_search_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Đọc log search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dữ liệu log_search có data 2 tháng, tháng 6 & 7 (14 ngày đầu tháng 6 & 14 ngày đầu tháng 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# parent_folder = 'E:/Dataset/log_search'\n",
    "# os.listdir(parent_folder)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------+--------------------+--------+---------+-------------------+-----------+------+--------------------+----------+\n",
      "|             eventID|            datetime| user_id|             keyword|category|proxy_isp|           platform|networkType|action|        userPlansMap|      date|\n",
      "+--------------------+--------------------+--------+--------------------+--------+---------+-------------------+-----------+------+--------------------+----------+\n",
      "|750fef93-60c8-402...|2022-07-13 20:00:...|06229349|ban ket 2 u19 tha...|   enter|     vnpt|            android|       wifi|search|                  []|2022-07-13|\n",
      "|523ff747-8f6b-426...|2022-07-13 20:00:...|    NULL|phim khuc vu thie...|   enter|      fpt|            android|       wifi|search|                NULL|2022-07-13|\n",
      "|f4fe6d50-4153-4ae...|2022-07-13 20:00:...|06189742|https://xembd.org...|   enter|      fpt|            android|       wifi|search|                  []|2022-07-13|\n",
      "|d39414ca-f816-4b7...|2022-07-13 20:00:...|06237501|bóng đá trực tiếp...|   enter|  viettel|            android|       wifi|search|                  []|2022-07-13|\n",
      "|b9e6c2e1-1e11-48b...|2022-07-13 20:00:...|    NULL|     lào vs thái lan|   enter|  viettel|            android|       wifi|search|                NULL|2022-07-13|\n",
      "|f4b48c93-c4a8-46b...|2022-07-13 20:00:...|06291395|u19 Thái Lan  _  ...|   enter|     vnpt|            android|       wifi|search|                  []|2022-07-13|\n",
      "|07655d12-f9b7-407...|2022-07-13 20:00:...|45696597|   TRUC TIEP BONG DA|   enter|  viettel|smarttv-tcl-android|       wifi|search|        [MAX:direct]|2022-07-13|\n",
      "|e3433dea-ad09-4ec...|2022-07-13 20:00:...|49439227|           thái lan |    quit|      fpt|            android|       wifi|search|                  []|2022-07-13|\n",
      "|e88bbfbc-1c49-4ad...|2022-07-13 20:00:...|06130154|trận bán kết U19 ...|   enter|     vnpt|            android|       wifi|search|                  []|2022-07-13|\n",
      "|ae6aa382-a427-441...|2022-07-13 20:00:...|49862814|   u19thai và u19lao|   enter|  viettel|            android|         3g|search|                  []|2022-07-13|\n",
      "|540b6474-a955-4e8...|2022-07-13 20:00:...|06628742|trực tiếp u 19 là...|    quit|     vnpt|            android|       wifi|search|                  []|2022-07-13|\n",
      "|383e02db-08d4-413...|2022-07-13 20:00:...|    NULL|             phim Mỹ|   enter|      fpt|            android|       wifi|search|                NULL|2022-07-13|\n",
      "|460fcf99-1bc6-4bd...|2022-07-13 20:00:...|41144891|            destined|   enter|  viettel|            android|         3g|search|[MAX:direct, Kênh...|2022-07-13|\n",
      "|e5ed066a-3aef-414...|2022-07-13 20:00:...|93757696|hoa nở trăng vừa ...|   enter|      fpt|                ios|       WIFI|search|                  []|2022-07-13|\n",
      "|40d527a2-3a0f-4b9...|2022-07-13 20:00:...|90055978|u19 việt nam - u1...|    quit|  viettel|smarttv-ceb-nextgen|       NULL|search|                  []|2022-07-13|\n",
      "|984ec8e6-f900-406...|2022-07-13 20:00:...|06623807|trực tiếp U19 Thá...|   enter|  viettel|            android|       wifi|search|                  []|2022-07-13|\n",
      "|8a957070-d805-486...|2022-07-13 20:00:...|06295482|       chung kết u19|   enter|     vnpt|            android|       wifi|search|                  []|2022-07-13|\n",
      "|c3a56654-cc7f-43f...|2022-07-13 20:00:...|    NULL|     Lào vs Thái Lan|   enter|  viettel|        web-playfpt|       NULL|search|                NULL|2022-07-13|\n",
      "|f4e821f1-3bea-4c1...|2022-07-13 13:00:...|95986420|               gecko|   enter|  viettel|    smart-tv-normal|       wifi|search|                  []|2022-07-13|\n",
      "|5b6b9772-231d-4b4...|2022-07-13 20:00:...| 1365997|thai lan gap lao ...|   enter|     vnpt|            android|       wifi|search|                  []|2022-07-13|\n",
      "+--------------------+--------------------+--------+--------------------+--------+---------+-------------------+-----------+------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parent_folder = 'E:/Dataset/log_search'\n",
    "df = spark.read.parquet(f\"{parent_folder}/*/*.parquet\") \\\n",
    "    .withColumn('date', to_date(col('datetime')))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('action').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_folder = 'E:/Dataset/log_search'\n",
    "file_name = f\"{parent_folder}/20220601/*.parquet\"\n",
    "df = spark.read.parquet(file_name)\n",
    "\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where('datetime is null').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre = df.withColumn('datetime_new', to_timestamp(col('datetime')))\n",
    "df_pre.where('datetime_new is null').select('datetime', 'datetime_new').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre.select('datetime', 'datetime_new').show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('platform').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Field giá trị là keyword và datetime, có thể là cả platform, đại diện cho \"user search keyword gì vào thời gian nào trên platform nào\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find most search trong ngày\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count_keyword = df.where(\"user_id is not null AND keyword is not null\") \\\n",
    "    .groupBy('user_id', 'keyword') \\\n",
    "    .count() \\\n",
    "    .orderBy(col('count').desc()) \n",
    "    \n",
    "df_count_keyword.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count_keyword.createOrReplaceTempView('keyword_count')\n",
    "\n",
    "df_most_search = spark.sql(f\"\"\"\n",
    "    select user_id, keyword as most_search, frequency\n",
    "    from (\n",
    "        select user_id, keyword, count as frequency,\n",
    "            row_number() over (partition by user_id order by count desc) as rnk\n",
    "        from keyword_count\n",
    "    ) t\n",
    "    where rnk = 1\n",
    "    order by frequency desc\n",
    "\"\"\")\n",
    "\n",
    "df_most_search.show()\n",
    "\n",
    "spark.catalog.dropTempView(\"keyword_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_most_search.createOrReplaceTempView('most_search')\n",
    "\n",
    "df_top_users = spark.sql(f\"\"\"\n",
    "    select distinct(user_id), frequency\n",
    "    from most_search\n",
    "    order by frequency desc\n",
    "    limit 1000\n",
    "\"\"\")\n",
    "\n",
    "df_top_users.show()\n",
    "\n",
    "spark.catalog.dropTempView(\"most_search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_most_search = df_most_search.join(df_top_users, ['user_id'])\n",
    "df_top_most_search.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Classify search text into categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from pyvi.ViTokenizer import tokenize\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('VoVanPhuc/sup-SimCSE-VietNamese-phobert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_keywords = [row[0] for row in df_count_keyword.select('keyword').collect()]\n",
    "list_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {\n",
    "    'Animation': ['DOREMON', 'MINECRAFT', 'BOBOIBOY', 'fairy tail', 'BORUTO', 'NARUTO', 'INUYASA', 'TSUBASA', 'FAIRY', 'LBX'],\n",
    "    'Music': ['ZUMBA', 'HOOWOO', 'KADAOKE', 'EDM', 'KARAOKE'],\n",
    "    'Entertainment': ['JOJO', 'mr. queen', 'GONJIAM', 'cuộc chiến thượng lưu', 'SIXT', 'CID', 'SVSV388', 'DAD NEEG'],\n",
    "    'TV Shows/Movies': ['paw', 'MAIKA', 'ANNE', 'bác sĩ ma', 'BAKI', 'boku no hero academia (season 2)', 'tìm mẹ'],\n",
    "    'Sport': ['COIDABANHTRUCTIEP', 'TRUCTIEPBONGDAHOMNAY'],\n",
    "    'Others': ['FPT', 'LUFF', 'DMSS', 'HOPE', 'hậu duệ mặt trời', 'HAOLAM', 'SCTV9', 'kênh vtv6', 'YUON', 'KPM']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify similar search text into the same category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Impossible, take too long to run (1hr for a day of search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ref: https://huggingface.co/VoVanPhuc/sup-SimCSE-VietNamese-phobert-base\n",
    "- Example notebook: https://colab.research.google.com/drive/12__EXJoQYHe9nhi4aXLTf9idtXT8yr7H?usp=sharing#scrollTo=5Dv9v66PwTLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- User có thể search khác keyword nhưng chúng có thể đồng nghĩa nhau. VD: 'trữ tình' & 'tru tinh', 'tình yêu' & 'yêu đương'. Để giải quyết, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from pyvi.ViTokenizer import tokenize\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "def clustering(embeddings, num_clusters):\n",
    "    model = KMeans(n_clusters=num_clusters)\n",
    "    model.fit(embeddings)\n",
    "    return model.labels_\n",
    "\n",
    "model = SentenceTransformer('VoVanPhuc/sup-SimCSE-VietNamese-phobert-base')\n",
    "sentences = all_search_text\n",
    "n_categories = 10\n",
    "\n",
    "# sentences = ['tây hành kỷ',\n",
    "#             'công chúa aurora',\n",
    "#           'nhiếp viễn',\n",
    "#           'taxi, em tên gì?',\n",
    "#           'nam cung phu nhân',\n",
    "#           'unforgettable love',\n",
    "#           ]\n",
    "\n",
    "sentences_tokenizer = [tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "embeddings = model.encode(sentences_tokenizer)\n",
    "\n",
    "cluster_labels = clustering(embeddings, n_categories).astype(str)\n",
    "\n",
    "print(f\"cluster_labels: {cluster_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = sentences[:6]\n",
    "# cluster_labels = list(cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "result_dict = {key: value for key, value in zip(sentences, cluster_labels)}\n",
    "\n",
    "csv_file_path = \"../temp/map_search_category.csv\"\n",
    "\n",
    "# Open the CSV file in write mode\n",
    "with open(csv_file_path, \"w+\", newline=\"\") as csv_file:\n",
    "    # Create a CSV writer object\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    \n",
    "    # Write the header row\n",
    "    csv_writer.writerow([\"text\", \"label\"])\n",
    "    \n",
    "    # Write the key-value pairs from the dictionary\n",
    "    for key, value in result_dict.items():\n",
    "        csv_writer.writerow([key, value])\n",
    "\n",
    "print(f\"Result saved to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create date dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calendar import monthcalendar\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import date\n",
    "import holidays\n",
    "\n",
    "def date_range(start_date, end_date):\n",
    "    start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    delta = timedelta(days=1)\n",
    "    dates_list = []\n",
    "    while start_date <= end_date:\n",
    "        dates_list.append(start_date.strftime('%Y-%m-%d'))\n",
    "        start_date += delta\n",
    "    return dates_list\n",
    "\n",
    "start_date = '2015-01-01'\n",
    "end_date = '2024-12-31'\n",
    "\n",
    "date_list = date_range(start_date, end_date)\n",
    "vn_holidays = holidays.VN()  # this is a dict\n",
    "\n",
    "def is_holiday(date):\n",
    "    return date in vn_holidays\n",
    "\n",
    "def get_holiday_name(date):\n",
    "    return vn_holidays.get(date)\n",
    "\n",
    "def get_week_of_month(year, month, day):\n",
    "    return next(\n",
    "        (\n",
    "            week_number\n",
    "            for week_number, days_of_week in enumerate(monthcalendar(year, month), start=1)\n",
    "            if day in days_of_week\n",
    "        ),\n",
    "        None,\n",
    "    )\n",
    "    \n",
    "udf_is_holiday = udf(is_holiday, BooleanType())\n",
    "udf_get_week_of_month = udf(get_week_of_month)\n",
    "udf_get_holiday_name = udf(get_holiday_name, StringType())\n",
    "\n",
    "df = spark.createDataFrame([(date,) for date in date_list], ['date']) \\\n",
    "    .withColumn('date', to_date(col('date'), 'yyyy-MM-dd')) \\\n",
    "    .withColumn(\"is_holiday\", udf_is_holiday(\"date\")) \\\n",
    "    .withColumn(\"quarter\", quarter(\"date\")) \\\n",
    "    .withColumn(\"year\", date_format(\"date\", \"yyyy\")) \\\n",
    "    .withColumn(\"date_key\", date_format(\"date\", \"yyyyMMdd\")) \\\n",
    "    .withColumn(\"week_of_month\", udf_get_week_of_month(year(col('date')), month(col('date')), dayofmonth(col('date')))) \\\n",
    "    .withColumn('holiday_name', when(col('is_holiday') == True, udf_get_holiday_name(col('date'))).otherwise(lit('work day')))\n",
    "    \n",
    "\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
